(.devops) tony@DESKTOP-JI2MQBC:~/dev/gitHub/DevOps_Microservices$ ./run_kubernetes.sh 
pod/ml-microservice created
NAME              READY   STATUS              RESTARTS   AGE
ml-microservice   0/1     ContainerCreating   0          0s
error: unable to forward port because pod is not running. Current status=Pending
(.devops) tony@DESKTOP-JI2MQBC:~/dev/gitHub/DevOps_Microservices$ kubectl port-forward ml-microservice 8000:80
Forwarding from 127.0.0.1:8000 -> 80
Forwarding from [::1]:8000 -> 80
Handling connection for 8000


Running make prediction in separate terminal window:

tony@DESKTOP-JI2MQBC:~/dev/gitHub/DevOps_Microservices$ ./make_prediction.sh
Port: 8000
{
  "prediction": [
    20.35373177134412
  ]
}


Resultant log output:
(Also see kubernates.png screenshot.  The prediction from the model is logged in the penultimate line)

2023-09-09 19:41:04  * Serving Flask app "app" (lazy loading)
2023-09-09 19:41:04  * Environment: production
2023-09-09 19:41:04    WARNING: Do not use the development server in a production environment.
2023-09-09 19:41:04    Use a production WSGI server instead.
2023-09-09 19:41:04  * Debug mode: on
2023-09-09 19:41:04  * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
2023-09-09 19:41:04  * Restarting with stat
2023-09-09 19:41:05  * Debugger is active!
2023-09-09 19:41:05  * Debugger PIN: 255-874-131
2023-09-10 16:25:04 [2023-09-10 15:25:04,226] INFO in app: JSON payload: 
2023-09-10 16:25:04 {'CHAS': {'0': 0}, 'RM': {'0': 6.575}, 'TAX': {'0': 296.0}, 'PTRATIO': {'0': 15.3}, 'B': {'0': 396.9}, 'LSTAT': {'0': 4.98}}
2023-09-10 16:25:04 [2023-09-10 15:25:04,334] INFO in app: Inference payload DataFrame: 
2023-09-10 16:25:04    CHAS     RM    TAX  PTRATIO      B  LSTAT
2023-09-10 16:25:04 0     0  6.575  296.0     15.3  396.9   4.98
2023-09-10 16:25:04 [2023-09-10 15:25:04,343] INFO in app: Scaling Payload: 
2023-09-10 16:25:04    CHAS     RM    TAX  PTRATIO      B  LSTAT
2023-09-10 16:25:04 0     0  6.575  296.0     15.3  396.9   4.98
2023-09-10 16:25:04 [2023-09-10 15:25:04,377] INFO in app: Prediction from Model: [20.35373177134412]
2023-09-10 16:25:04 127.0.0.1 - - [10/Sep/2023 15:25:04] "POST /predict HTTP/1.1" 200 -


Or if you like using the command line tool:
(I'm not sure what you are looking for here - the initial submission had the "Prediction from Model:" in it but is something more required?)

(.devops) tony@DESKTOP-JI2MQBC:~/dev/gitHub/DevOps_Microservices$ kubectl logs ml-microservice
 * Serving Flask app "app" (lazy loading)
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 255-874-131
[2023-09-10 15:25:04,226] INFO in app: JSON payload:
{'CHAS': {'0': 0}, 'RM': {'0': 6.575}, 'TAX': {'0': 296.0}, 'PTRATIO': {'0': 15.3}, 'B': {'0': 396.9}, 'LSTAT': {'0': 4.98}}
[2023-09-10 15:25:04,334] INFO in app: Inference payload DataFrame:
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2023-09-10 15:25:04,343] INFO in app: Scaling Payload:
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2023-09-10 15:25:04,377] INFO in app: Prediction from Model: [20.35373177134412]
127.0.0.1 - - [10/Sep/2023 15:25:04] "POST /predict HTTP/1.1" 200 -

